# Week 1
## Distributional hypothesis and Word2Vec

During this lesson we will cover the basics of distributional hypothesis, take some quick look at maximum likelihood and will understand why dense embeddings are better than one-hot embeddings.
### Link to seminar #1 in google colab:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RepTrak/nlp-course/blob/develop/Week1/seminar_1.ipynb)

### Link to homework #1 in google colab:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RepTrak/nlp-course/blob/develop/Week1/homework_1.ipynb)
