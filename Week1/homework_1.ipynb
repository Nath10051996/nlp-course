{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "homework_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKVzEKCW4JNS"
      },
      "source": [
        "# Homework 1: Word2vec + Negative Sampling\n",
        "\n",
        "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
        "\n",
        "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
        "\n",
        "It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
        "\n",
        "Example of visualization in tensorboard:\n",
        "https://projector.tensorflow.org\n",
        "\n",
        "Example of 2D visualisation:\n",
        "\n",
        "![2dword2vec](https://www.tensorflow.org/images/tsne.png)\n",
        "\n",
        "### Homework task: Use Negative Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ5rafQF0CCQ"
      },
      "source": [
        "## Theory\n",
        "\n",
        "There are two types of word to vec.\n",
        "\n",
        "### Skipgram\n",
        "\n",
        "Predicting outside word $o$ from central $c$. We have two embedding mattrix $u$ and $v$.\n",
        "\n",
        "\n",
        "\n",
        "![](https://i.ibb.co/xgT4k8b/2020-10-02-10-10-21.png)\n",
        "\n",
        "$P(o \\mid c)=\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}$.\n",
        "\n",
        "More formally we need to maximize Likelohood:\n",
        "$$\n",
        "L(\\theta)=\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P\\left(w_{t+j} \\mid w_{t}, \\theta\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "L_{\\log}(\\theta) = \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P\\left(w_{t+j} \\mid w_{t}, \\theta\\right) = \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log \\frac{\\exp \\left(u_{t+j}^{T} v_{t}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{t}\\right)} = \\\\ = \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} u_{t+j}^{T} v_{t} - \\log \\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{t}\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "loss = -L_{\\log}\n",
        "$$\n",
        "\n",
        "Let's count derivative!\n",
        "\n",
        "**Reminder**\n",
        "\n",
        "$$\\frac{\\partial x^T y}{\\partial y} = x$$\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L_{log}(\\theta)}{\\partial v_t} = u_o - \\dfrac{1}{\\sum_w\\exp(u_w^T v_t)}\\cdot\\sum_x \\exp(u_x^T v_t) u_x = u_o - \\sum_x \\frac{\\exp(u_x v_t)}{\\sum_w \\exp(u_w v_t)} u_x = \\\\ = u_0 - \\sum_x P(u_x| v_t) u_x\n",
        "$$\n",
        "\n",
        "### CBOW\n",
        "\n",
        "![](https://lena-voita.github.io/resources/lectures/word_emb/w2v/cbow_skip-min.png)\n",
        "\n",
        "## Practice\n",
        "\n",
        "### Variant 1\n",
        "\n",
        "```python\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, voc_size, emb_dim):\n",
        "        self.u = nn.Embedding(voc_size, emb_dim)\n",
        "        self.v = nn.Embedding(voc_size, emb_dim)\n",
        "\n",
        "w2v = Model(...)\n",
        "\n",
        "def step(word, context):\n",
        "    for c_word in context:\n",
        "        loss = - w2v.u(word).T.dot(w2v.v(c_word))\n",
        "        cum_exp = 0\n",
        "        for i in range(voc_size):\n",
        "            if i == c_word:\n",
        "                continue\n",
        "            cum_exp += w2v.u(word).T.dot(w2v.v(c_word)).exp()\n",
        "        loss += torch.log(cum_exp)\n",
        "        loss.backward()\n",
        "        ...\n",
        "```\n",
        "\n",
        "### Variant 2\n",
        "\n",
        "![](https://i.ibb.co/qydjBbv/2020-10-02-12-16-33.png)\n",
        "\n",
        "```python\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, voc_size, emb_dim):\n",
        "        self.u = nn.Embedding(voc_size, emb_dim)\n",
        "        self.v = nn.Linear(emb_dim, voc_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.v(self.u(x))\n",
        "\n",
        "w2v = Model(...)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def step(word, context):\n",
        "    for c_word in context:\n",
        "        preds = w2v(word)\n",
        "        loss = criterion(preds, c_word)\n",
        "        loss.backward()\n",
        "        ...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efm3r3cF4AWy"
      },
      "source": [
        "## Homework #1\n",
        "\n",
        "### Negative sampling\n",
        "\n",
        "Instead of updating all context vectors we can sample 5-10.\n",
        "\n",
        "$$\n",
        "loss = -\\log \\sigma\\left(u_{context}^{T} v_{center}\\right)-\\sum_{w \\in\\left\\{w_{i_{1}}, \\ldots, w_{i_{K}}\\right\\}} \\log \\sigma\\left(-u_{w}^{T} v_{center}\\right)\n",
        "$$\n",
        "\n",
        "**How can we sample negative words?**\n",
        "\n",
        "According to their probability: $p_{sample} (w) = p_{word} (w) ^{3/4}$\n",
        "\n",
        "Information $= -\\log(P)$\n",
        "\n",
        "\n",
        "### Not all of the words are equally important\n",
        "\n",
        "### Distance between center and context\n",
        "\n",
        "![](https://lena-voita.github.io/resources/lectures/word_emb/research/w2v_position-min.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK9h1kvw4L6j"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import itertools\n",
        "import nltk\n",
        "import re\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from itertools import zip_longest\n",
        "from string import ascii_lowercase\n",
        "import snowballstemmer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ5Zs6GtCUGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e21a2e1-d67f-4910-ffba-334b5f3a70bb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cvvxgax4M5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8029db8-16bd-459a-a01a-41a6cd0c1238"
      },
      "source": [
        "!wget http://mattmahoney.net/dc/text8.zip\n",
        "!unzip /content/text8.zip -d text8/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-12 00:39:59--  http://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.24\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.24|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip’\n",
            "\n",
            "text8.zip           100%[===================>]  29.89M   721KB/s    in 43s     \n",
            "\n",
            "2021-03-12 00:40:42 (712 KB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
            "\n",
            "Archive:  /content/text8.zip\n",
            "  inflating: text8/text8             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON1mdxpn4WPY"
      },
      "source": [
        "with open('/content/text8/text8', 'r') as txt_file:\n",
        "    text_8 = txt_file.read()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ux_0Erv5Pyu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "896d5c91-4af9-4e3b-d1bd-91860ab62fea"
      },
      "source": [
        "print(len(text_8))\n",
        "text_8[0:500]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIfrBOW95UGs"
      },
      "source": [
        "class SkipGramBatcher(object):\n",
        "    def __init__(self, text, window_size = 3, batch_size = 2048, vocab_size=60000):\n",
        "        pass\n",
        "        # your code goes here"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IP-BSp89qjl"
      },
      "source": [
        "class SkipGramNegativeSampling(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        pass\n",
        "        # your code goes here"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDPkzMLPG3zK"
      },
      "source": [
        "criterion = # your code goes here\n",
        "optimizer = # your code goes here\n",
        "scheduler = # your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7hsq2zilBNr"
      },
      "source": [
        "epochs = # your code goes here\n",
        "train_losses = # your code goes here\n",
        "verbose_at_batch = # your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OumuIfng6Xg"
      },
      "source": [
        "def train_model(model, loss_function, optimizer, scheduler, train_losses, verbose_at_batch, epochs):\n",
        "    # your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}